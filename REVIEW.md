# Model Review: Encoder-Decoder PINN for Geophysical Inversion

## What the model does well

- The overall architecture (encoder-decoder + PINN constraint) is sound for this inverse problem.
- Using ILR to handle compositional closure is the correct approach. The Helmert contrast matrix is correctly constructed (rows are orthonormal, sum to zero).
- Coordinate normalization for the ILR decoder is appropriate.
- The reparameterization trick for VAE sampling is standard and correct.
- Using JAX's `vmap` and `jit` for the forward model is idiomatic.

---

## Reliability Issues

### 1. Forward Model Mismatch (Critical)

**Location:** `exercice.py:204-222`

The JAX forward model is a simplified vertical dipole approximation, but the training data is generated by SimPEG's full 3D integral method. This creates a **systematic bias** in the PINN loss:

- The dipole formula `(3*cos_theta^2 - 1) / r^3` only considers the vertical component, but the Earth's field has inclination 70° (not 90°). The actual TMI anomaly depends on the projection of the anomalous field onto the inducing field direction.
- SimPEG computes a full integral over prism cells. The dipole approximation treats each cell as a point source, which breaks down for nearby cells.
- The model is forced to learn a susceptibility field that **compensates for the forward model error** rather than recovering the true geology.

**Fix:** Use SimPEG's sensitivity matrix `G` (the forward problem is linear: `mag = G @ chi`) and wrap it for JAX:

```python
G_jax = jnp.array(sim.G)

@jit
def forward_magnetic_jax(susceptibility):
    return G_jax @ susceptibility
```

### 2. No Spatial Regularization (High)

**Location:** `exercice.py:290-338`

The loss function has no smoothness constraint on the predicted susceptibility or compositions. The inverse problem is severely ill-posed (144 measurements for 2048 unknowns). Without regularization, the susceptibility field can develop physically unrealistic checkerboard patterns.

**Fix:** Add a spatial smoothness penalty:

```python
def smoothness_loss(chi, nx, ny, nz):
    chi_3d = chi.reshape(nz, ny, nx)
    dx = jnp.sum((chi_3d[:, :, 1:] - chi_3d[:, :, :-1])**2)
    dy = jnp.sum((chi_3d[:, 1:, :] - chi_3d[:, :-1, :])**2)
    dz = jnp.sum((chi_3d[1:, :, :] - chi_3d[:-1, :, :])**2)
    return (dx + dy + dz) / chi.size
```

### 3. Encoder Ignores Spatial Structure (High)

**Location:** `exercice.py:237-250`

The encoder flattens the 12×12 magnetic grid through Dense layers, discarding the 2D spatial structure. The network cannot learn that adjacent stations measure correlated signals.

**Fix:** Replace the MLP encoder with a CNN:

```python
class Encoder(nn.Module):
    latent_dim: int = 32

    @nn.compact
    def __call__(self, mag_grid):
        x = mag_grid[jnp.newaxis, :, :, jnp.newaxis]  # (1, 12, 12, 1)
        x = nn.Conv(16, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.relu(x)
        x = nn.Conv(32, kernel_size=(3, 3), padding='SAME')(x)
        x = nn.relu(x)
        x = x.ravel()
        x = nn.Dense(64)(x)
        x = nn.relu(x)
        mu = nn.Dense(self.latent_dim)(x)
        log_var = nn.Dense(self.latent_dim)(x)
        return mu, log_var
```

### 4. Susceptibility Decoder Bottleneck (Medium-High)

**Location:** `exercice.py:253-267`

The decoder maps 32-dim latent → 2048 outputs through a max of 256 hidden units. The final layer performs most of the work with very limited capacity.

**Fix:** Use wider intermediate layers (512, 1024), or reshape into a 3D volume and use transposed convolutions for spatially coherent output.

### 5. Hardcoded Susceptibility Scaling (Medium)

**Location:** `exercice.py:267`

`jnp.exp(log_chi) * 0.01` — the `0.01` factor is fragile. If the true susceptibility range changes, this breaks. The `exp` transform is also unbounded, which can cause numerical issues.

**Fix:** Use `softplus` with data-driven scaling, or normalize targets and let the network learn the scale.

### 6. No Positional Encoding for ILR Decoder (Medium)

**Location:** `exercice.py:270-283`

MLPs with ReLU struggle with high-frequency spatial functions from raw (x,y,z) inputs ("spectral bias"). The ILR decoder receives raw normalized coordinates.

**Fix:** Add Fourier feature encoding:

```python
def fourier_features(coords, n_freqs=6):
    freqs = 2.0 ** jnp.arange(n_freqs)
    x = coords[..., jnp.newaxis] * freqs
    x = x.reshape(*coords.shape[:-1], -1)
    return jnp.concatenate([jnp.sin(x), jnp.cos(x)], axis=-1)
```

### 7. Non-Reproducible Noise (Medium)

**Location:** `exercice.py:159`

`np.random.randn` is used without a seed, while the rest of the code uses `jax.random.PRNGKey(42)`. The experiment is not fully reproducible.

**Fix:**

```python
rng_np = np.random.default_rng(42)
mag_observed = sim.dpred(susceptibility_true) + rng_np.normal(0, 1.5, len(stations))
```

### 8. No Validation Strategy (Medium)

There is no train/test split, no held-out boreholes, and no quantitative metrics (R², RMSE). There is no way to assess whether the model generalizes or overfits.

**Fix:** Hold out 2–3 boreholes for validation. Compute RMSE and R² on both training and validation sets. Plot predicted vs. true at held-out locations.

### 9. KL Weight Too Low / No Annealing (Low-Medium)

**Location:** `exercice.py:394`

With `lambda_kl=0.001`, the latent space may collapse to a point estimate (posterior ignores the prior), making uncertainty estimates unreliable.

**Fix:** Use KL annealing — start with `lambda_kl=0` and linearly increase to a target (0.01–0.1) over the first 500 epochs. Monitor `log_var` during training.

### 10. Fixed Learning Rate (Low-Medium)

**Location:** `exercice.py:384`

A fixed learning rate of `1e-3` for 2000 epochs may cause oscillation near the optimum.

**Fix:** Use a cosine or step schedule via `optax.warmup_cosine_decay_schedule`.

### 11. Shallow Uncertainty Quantification (Low-Medium)

**Location:** `exercice.py:456-476`

Drawing 30 latent samples captures only **latent uncertainty**. Epistemic uncertainty (network weights), aleatoric uncertainty (data noise), and model uncertainty (forward model error) are not accounted for.

**Fix:** Consider MC Dropout or deep ensembles. At minimum, increase samples to 100+ and check convergence.

### 12. No Noise on Borehole Data (Low)

**Location:** `exercice.py:86-90`

Compositions are taken directly from the synthetic model without noise. Real borehole assays have analytical uncertainty (typically 5–15% relative).

**Fix:** Add realistic noise to borehole compositions for more robust training.

---

## Summary

| # | Issue | Severity | Effort |
|---|---|---|---|
| 1 | Forward model mismatch | Critical | Low |
| 2 | No spatial regularization | High | Low |
| 3 | Encoder ignores spatial structure | High | Medium |
| 4 | Decoder susceptibility bottleneck | Medium-High | Low |
| 5 | Hardcoded susceptibility scaling | Medium | Low |
| 6 | No positional encoding | Medium | Low |
| 7 | Non-reproducible noise | Medium | Trivial |
| 8 | No validation strategy | Medium | Low |
| 9 | KL weight / annealing | Low-Medium | Low |
| 10 | Fixed learning rate | Low-Medium | Trivial |
| 11 | Shallow uncertainty quantification | Low-Medium | Medium |
| 12 | No noise on borehole data | Low | Trivial |

## Recommended Priority

1. **Fix the forward model** — single most impactful change; a biased physics constraint systematically misleads the inversion
2. **Add spatial regularization** — essential for ill-posed inverse problems
3. **Add validation** — necessary to know if the model works
4. **CNN encoder** — respect spatial structure of inputs
5. **Positional encoding + wider decoder** — improve expressiveness
6. **KL annealing + LR schedule** — better training dynamics
7. **Reproducibility fixes** — seed numpy RNG
